{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import gc\n"
   ],
   "metadata": {
    "id": "Sk3u7UimFeSc",
    "ExecuteTime": {
     "end_time": "2025-06-04T17:27:58.999786Z",
     "start_time": "2025-06-04T17:27:58.982189Z"
    }
   },
   "outputs": [],
   "execution_count": 54
  },
  {
   "source": [
    "file_path = '../data/labeled_sunset.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"File loaded successfully. First 5 rows:\")\n",
    "print(df.head())"
   ],
   "cell_type": "code",
   "metadata": {
    "id": "krIFqP_fFaRz",
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:19.582363Z",
     "start_time": "2025-06-04T16:37:11.791274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_9880\\3763364777.py:4: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully. First 5 rows:\n",
      "   Unnamed: 0              userid       username  \\\n",
      "0           3            22240612  AlArabiya_Eng   \n",
      "1           4             6135622     dw_espanol   \n",
      "2           5  848416437030985728   ChangshaCity   \n",
      "3           8  984429894829592576   pulsoguayaco   \n",
      "4           9  807095565028917248  linjianyangbe   \n",
      "\n",
      "                                            acctdesc  \\\n",
      "0  The Arab world‚Äôs leading source of global news...   \n",
      "1  Desde Alemania para Am√©rica Latina. Todo lo qu...   \n",
      "2  Changsha, the capital of central China‚Äôs Hunan...   \n",
      "3  üåê‚úàBlog de aviaci√≥n, viajes y econom√≠a para via...   \n",
      "4  Nature heals. Birding in China: best photos+vi...   \n",
      "\n",
      "                     location  following  followers  totaltweets  \\\n",
      "0                       Dubai         46     921780       324925   \n",
      "1             Berlin, Germany        160    1266110       157669   \n",
      "2  People's Republic of China        261      47826         3634   \n",
      "3                        üá™üá®üá∫üá∏         75        326        14487   \n",
      "4                       China      12629      19112         6718   \n",
      "\n",
      "                usercreatedts              tweetid  ... in_reply_to_status_id  \\\n",
      "0  2009-02-28 08:31:32.000000  1567301623294164997  ...                     0   \n",
      "1  2007-05-18 11:40:24.000000  1567301623310913538  ...                     0   \n",
      "2  2017-04-02 06:06:56.000000  1567301623772110848  ...                     0   \n",
      "3  2018-04-12 13:55:51.000000  1567301624531275776  ...                     0   \n",
      "4  2016-12-09 05:32:32.000000  1567301624967618562  ...                     0   \n",
      "\n",
      "   in_reply_to_user_id in_reply_to_screen_name is_quote_status  \\\n",
      "0                    0                     NaN           False   \n",
      "1                    0                     NaN           False   \n",
      "2                    0                     NaN           False   \n",
      "3                    0                     NaN           False   \n",
      "4                    0                     NaN           False   \n",
      "\n",
      "  quoted_status_id quoted_status_userid  quoted_status_username  \\\n",
      "0                0                    0                     NaN   \n",
      "1                0                    0                     NaN   \n",
      "2                0                    0                     NaN   \n",
      "3                0                    0                     NaN   \n",
      "4                0                    0                     NaN   \n",
      "\n",
      "                  extractedts            id  label  \n",
      "0  2022-09-07 05:01:02.893020  2.224061e+07  human  \n",
      "1  2022-09-07 10:06:45.585367  6.135622e+06  human  \n",
      "2  2022-09-07 03:15:12.921515  8.484164e+17  human  \n",
      "3  2022-09-07 10:06:45.564494  9.844299e+17  human  \n",
      "4  2022-09-07 03:15:12.800454  8.070956e+17  human  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Drop useless columns #"
  },
  {
   "source": [
    "cols_to_drop = [\n",
    "    'username', 'location', 'usercreatedts', 'tweetid', 'hashtags', 'coordinates',\n",
    "    'favorite_count', 'is_retweet', 'original_tweet_id', 'original_tweet_userid', 'original_tweet_username',\n",
    "    'in_reply_to_status_id', 'in_reply_to_user_id', 'in_reply_to_screen_name', 'is_quote_status',\n",
    "    'quoted_status_id', 'quoted_status_userid','extractedts' ,'quoted_status_username', 'id'\n",
    "]\n",
    "\n",
    "df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "gc.collect()\n",
    "print(\"Specified columns dropped and memory cleaned up.\")\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col)"
   ],
   "cell_type": "code",
   "metadata": {
    "id": "qJ8EEwk6Fz69",
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:30.898078Z",
     "start_time": "2025-06-04T16:38:19.746543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified columns dropped and memory cleaned up.\n",
      "Unnamed: 0\n",
      "userid\n",
      "acctdesc\n",
      "following\n",
      "followers\n",
      "totaltweets\n",
      "tweetcreatedts\n",
      "retweetcount\n",
      "text\n",
      "language\n",
      "label\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get unique users #"
   ],
   "metadata": {
    "id": "FhfdlSA-S2Tn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "unique_user_ids = df['userid'].unique()\n",
    "# Create a new DataFrame with the unique user IDs\n",
    "proc_df = pd.DataFrame({'userid': unique_user_ids})\n",
    "\n",
    "print(\"New DataFrame created with unique user IDs:\")\n",
    "print(proc_df.head())\n",
    "print(f\"Total unique users: {len(proc_df)}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "Mb5UfLjGS16M",
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:55.346683Z",
     "start_time": "2025-06-04T16:38:55.256995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame created with unique user IDs:\n",
      "               userid\n",
      "0            22240612\n",
      "1             6135622\n",
      "2  848416437030985728\n",
      "3  984429894829592576\n",
      "4  807095565028917248\n",
      "Total unique users: 104288\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Total tweets is max of the total tweets for each user #\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "sgT1b2AnUqrm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Group the original DataFrame by 'userid' and find the maximum 'totaltweets' for each user\n",
    "max_total_tweets = df.groupby('userid')['totaltweets'].max().reset_index()\n",
    "\n",
    "# Merge the results into the 'proc_df'\n",
    "proc_df = pd.merge(proc_df, max_total_tweets, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'totaltweets':\")\n",
    "print(proc_df.head())"
   ],
   "metadata": {
    "id": "jq33MXU2UxBy",
    "ExecuteTime": {
     "end_time": "2025-06-04T16:39:26.233778Z",
     "start_time": "2025-06-04T16:39:26.059593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'totaltweets':\n",
      "               userid  totaltweets\n",
      "0            22240612       347147\n",
      "1             6135622       172270\n",
      "2  848416437030985728         3634\n",
      "3  984429894829592576        16116\n",
      "4  807095565028917248         8325\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "id": "PkLo4C43W5Bm"
   },
   "cell_type": "markdown",
   "source": [
    "# Following and Followers from the latest tweet #\n",
    "* followers_following_ratio = following/followers\n",
    "* if followers_following_ratio is not defined it will be the number of following"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:39:45.837232Z",
     "start_time": "2025-06-04T16:39:33.578938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sort df by userid and tweetcreatedts (latest first)\n",
    "df_sorted = df.sort_values(by=['userid', 'tweetcreatedts'], ascending=[True, False])\n",
    "\n",
    "# Get the latest tweet for each user (first row after sorting)\n",
    "latest_tweets = df_sorted.groupby('userid').first().reset_index()\n",
    "\n",
    "# Select relevant columns\n",
    "latest_user_info = latest_tweets[['userid', 'following', 'followers']]\n",
    "\n",
    "# Calculate the followers_following_ratio, handling potential division by zero\n",
    "latest_user_info['followers_following_ratio'] = latest_user_info.apply(\n",
    "    lambda row: row['following'] / row['followers'] if row['followers'] != 0 else row['following'], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Merge with proc_df\n",
    "proc_df = pd.merge(proc_df, latest_user_info, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with latest tweet info, following, followers, and ratio:\")\n",
    "print(proc_df.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with latest tweet info, following, followers, and ratio:\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  \n",
      "0                   0.000050  \n",
      "1                   0.000124  \n",
      "2                   0.005457  \n",
      "3                   0.219512  \n",
      "4                   0.614297  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_9880\\1647166422.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  latest_user_info['followers_following_ratio'] = latest_user_info.apply(\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "id": "mzJqyuO8i91k"
   },
   "cell_type": "markdown",
   "source": "# avg_retweetcount #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:42:28.890603Z",
     "start_time": "2025-06-04T16:42:28.661221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Group the original DataFrame by 'userid' and calculate the average 'retweetcount' for each user\n",
    "temp_df = df.groupby('userid')['retweetcount'].mean().reset_index()\n",
    "\n",
    "# Rename the column to be more descriptive before merging (optional but good practice)\n",
    "temp_df.rename(columns={'retweetcount': 'avg_retweetcount'}, inplace=True)\n",
    "\n",
    "# Merge the results into the 'proc_df'\n",
    "proc_df = pd.merge(proc_df, temp_df, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'avg_retweetcount':\")\n",
    "print(proc_df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'avg_retweetcount':\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  avg_retweetcount  \n",
      "0                   0.000050          1.755378  \n",
      "1                   0.000124         39.639711  \n",
      "2                   0.005457          1.000000  \n",
      "3                   0.219512          0.398857  \n",
      "4                   0.614297         18.340000  \n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "id": "f_7EdkDJjCf8"
   },
   "cell_type": "markdown",
   "source": "# Avg_words_per_tweet #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:43:15.860438Z",
     "start_time": "2025-06-04T16:42:41.541387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate word count for each tweet\n",
    "# We'll handle potential NaN values in the 'text' column\n",
    "df['word_count'] = df['text'].str.split().str.len().fillna(0)\n",
    "\n",
    "# Group by 'userid' and calculate the average word count\n",
    "temp_df = df.groupby('userid')['word_count'].mean().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "temp_df.rename(columns={'word_count': 'avg_words_per_tweet'}, inplace=True)\n",
    "\n",
    "# Merge into proc_df\n",
    "proc_df = pd.merge(proc_df, temp_df, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'avg_words_per_tweet':\")\n",
    "print(proc_df.head())\n",
    "\n",
    "# delete word_count from df\n",
    "if 'word_count' in df.columns:\n",
    "    del df['word_count']\n",
    "    print(\"Deleted 'word_count' from df\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'avg_words_per_tweet':\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  avg_retweetcount  avg_words_per_tweet  \n",
      "0                   0.000050          1.755378            30.008004  \n",
      "1                   0.000124         39.639711            35.051940  \n",
      "2                   0.005457          1.000000            45.000000  \n",
      "3                   0.219512          0.398857            29.329143  \n",
      "4                   0.614297         18.340000            24.560000  \n",
      "Deleted 'word_count' from df\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# daily_tweet_count #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:44:26.764913Z",
     "start_time": "2025-06-04T16:44:23.284158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure 'tweetcreatedts' is in datetime format\n",
    "df['tweetcreatedts'] = pd.to_datetime(df['tweetcreatedts'])\n",
    "\n",
    "# Extract the date from 'tweetcreatedts'\n",
    "df['tweet_date'] = df['tweetcreatedts'].dt.date\n",
    "\n",
    "# Group by userid and date, then count tweets per day\n",
    "temp_df = df.groupby(['userid', 'tweet_date']).size().reset_index(name='daily_tweet_count')\n",
    "\n",
    "# Group by userid and calculate the average daily tweet count\n",
    "tweet_frequency_per_day = temp_df.groupby('userid')['daily_tweet_count'].mean().reset_index()\n",
    "\n",
    "# Merge with proc_df\n",
    "proc_df = pd.merge(proc_df, tweet_frequency_per_day, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'tweet_frequency_per_day':\")\n",
    "print(proc_df.head())\n",
    "\n",
    "if 'tweet_frequency_per_day' in globals():\n",
    "    del tweet_frequency_per_day\n",
    "    print(\"Deleted 'tweet_frequency_per_day'\")\n",
    "\n",
    "if 'tweet_date' in df.columns:\n",
    "    del df['tweet_date']\n",
    "    print(\"Deleted 'tweet_date' from df\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'tweet_frequency_per_day':\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  avg_retweetcount  avg_words_per_tweet  \\\n",
      "0                   0.000050          1.755378            30.008004   \n",
      "1                   0.000124         39.639711            35.051940   \n",
      "2                   0.005457          1.000000            45.000000   \n",
      "3                   0.219512          0.398857            29.329143   \n",
      "4                   0.614297         18.340000            24.560000   \n",
      "\n",
      "   daily_tweet_count  \n",
      "0          14.077465  \n",
      "1           5.318182  \n",
      "2           1.000000  \n",
      "3           4.729730  \n",
      "4           1.785714  \n",
      "Deleted 'tweet_frequency_per_day'\n",
      "Deleted 'tweet_date' from df\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# unique_language_count #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:44:42.481231Z",
     "start_time": "2025-06-04T16:44:41.910934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Group by userid and language and count the occurrences\n",
    "temp_df = df.groupby(['userid', 'language']).size().reset_index(name='language_count')\n",
    "\n",
    "# Group by userid and count the number of unique languages\n",
    "unique_language_count = temp_df.groupby('userid').size().reset_index(name='unique_language_count')\n",
    "\n",
    "# Merge with proc_df\n",
    "proc_df = pd.merge(proc_df, unique_language_count, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'unique_language_count':\")\n",
    "print(proc_df.head())\n",
    "\n",
    "\n",
    "if 'unique_language_count' in globals():\n",
    "    del unique_language_count\n",
    "    print(\"Deleted 'unique_language_count'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'unique_language_count':\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  avg_retweetcount  avg_words_per_tweet  \\\n",
      "0                   0.000050          1.755378            30.008004   \n",
      "1                   0.000124         39.639711            35.051940   \n",
      "2                   0.005457          1.000000            45.000000   \n",
      "3                   0.219512          0.398857            29.329143   \n",
      "4                   0.614297         18.340000            24.560000   \n",
      "\n",
      "   daily_tweet_count  unique_language_count  \n",
      "0          14.077465                      2  \n",
      "1           5.318182                      2  \n",
      "2           1.000000                      1  \n",
      "3           4.729730                      2  \n",
      "4           1.785714                      4  \n",
      "Deleted 'unique_language_count'\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# max_tweers_per_hour #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T17:41:10.368745Z",
     "start_time": "2025-06-04T17:39:03.550461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure 'tweetcreatedts' is in datetime format\n",
    "df['tweetcreatedts'] = pd.to_datetime(df['tweetcreatedts'])\n",
    "# Sort df by userid and tweetcreatedts\n",
    "df_sorted = df.sort_values(by=['userid', 'tweetcreatedts'])\n",
    "\n",
    "# Count tweets per hour window per user\n",
    "burstiness = (\n",
    "    df_sorted\n",
    "    .groupby('userid')\n",
    "    .apply(lambda df: df.set_index('tweetcreatedts')\n",
    "                      .rolling('1h')['userid']\n",
    "                      .count()\n",
    "                      .max())\n",
    "    .reset_index(name='Max_tweets_in_1hr')\n",
    ")\n",
    "\n",
    "\n",
    "# Merge back\n",
    "proc_df = pd.merge(proc_df, burstiness, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'Max_tweets_in_1hr':\")\n",
    "print(proc_df.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'Max_tweets_in_1hr':\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  avg_retweetcount  avg_words_per_tweet  \\\n",
      "0                   0.000050          1.755378            30.008004   \n",
      "1                   0.000124         39.639711            35.051940   \n",
      "2                   0.005457          1.000000            45.000000   \n",
      "3                   0.219512          0.398857            29.329143   \n",
      "4                   0.614297         18.340000            24.560000   \n",
      "\n",
      "   daily_tweet_count  unique_language_count  Max_tweets_in_1hr  \n",
      "0          14.077465                      2                7.0  \n",
      "1           5.318182                      2                6.0  \n",
      "2           1.000000                      1                1.0  \n",
      "3           4.729730                      2                7.0  \n",
      "4           1.785714                      4                2.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_9880\\4035292727.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda df: df.set_index('tweetcreatedts')\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add label #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T18:15:37.801885Z",
     "start_time": "2025-06-04T18:15:37.262724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Group by userid and get the first occurrence of the label for each user\n",
    "temp_df = df.groupby('userid')['label'].first().reset_index(name='label')\n",
    "\n",
    "# Map labels: \"human\" to 0, \"bot\" to 1\n",
    "label_mapping = {\"human\": 0, \"bot\": 1}\n",
    "temp_df['label'] = temp_df['label'].map(label_mapping)\n",
    "\n",
    "# Merge the results into the 'proc_df'\n",
    "proc_df = pd.merge(proc_df, temp_df, on='userid', how='left')\n",
    "\n",
    "print(\"Updated 'proc_df' with 'label' (first instance and mapped):\")\n",
    "print(proc_df.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'proc_df' with 'label' (first instance and mapped):\n",
      "               userid  totaltweets  following  followers  \\\n",
      "0            22240612       347147         46     925487   \n",
      "1             6135622       172270        169    1367996   \n",
      "2  848416437030985728         3634        261      47826   \n",
      "3  984429894829592576        16116         72        328   \n",
      "4  807095565028917248         8325      15984      26020   \n",
      "\n",
      "   followers_following_ratio  avg_retweetcount  avg_words_per_tweet  \\\n",
      "0                   0.000050          1.755378            30.008004   \n",
      "1                   0.000124         39.639711            35.051940   \n",
      "2                   0.005457          1.000000            45.000000   \n",
      "3                   0.219512          0.398857            29.329143   \n",
      "4                   0.614297         18.340000            24.560000   \n",
      "\n",
      "   daily_tweet_count  unique_language_count  Max_tweets_in_1hr  label  \n",
      "0          14.077465                      2                7.0      0  \n",
      "1           5.318182                      2                6.0      0  \n",
      "2           1.000000                      1                1.0      0  \n",
      "3           4.729730                      2                7.0      0  \n",
      "4           1.785714                      4                2.0      0  \n",
      "Deleted 'user_labels_first'\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Memory cleanup #\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T18:18:20.199910Z",
     "start_time": "2025-06-04T18:18:20.179769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Deletes temporary variables to free up memory.\"\"\"\n",
    "if 'unique_user_ids' in globals():\n",
    "  del unique_user_ids\n",
    "  print(\"Deleted 'unique_user_ids'\")\n",
    "\n",
    "if 'max_total_tweets' in globals():\n",
    "  del max_total_tweets\n",
    "  print(\"Deleted 'max_total_tweets'\")\n",
    "\n",
    "if 'df_sorted' in globals():\n",
    "  del df_sorted\n",
    "  print(\"Deleted 'df_sorted'\")\n",
    "\n",
    "if 'temp_df' in globals():\n",
    "  del temp_df\n",
    "  print(\"Deleted 'temp_df'\")\n",
    "\n",
    "if 'latest_user_info' in globals():\n",
    "  del latest_user_info\n",
    "  print(\"Deleted 'latest_user_info'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 'temp_df'\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Make userdesc_labeled for the LM model #"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = '../data/labeled_sunset.csv'\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Map string labels to numeric\n",
    "df = df.drop_duplicates(subset=['id'], keep='first').copy()\n",
    "df_clean = df[[\"userid\", \"acctdesc\", \"label\"]].dropna()\n",
    "label_map = {\"human\": 0, \"bot\": 1}\n",
    "df_clean[\"label\"] = df_clean[\"label\"].map(label_map)\n",
    "\n",
    "# Check result\n",
    "print(df_clean[\"label\"].value_counts())\n",
    "\n",
    "# Drop rows with missing descriptions\n",
    "df_clean = df_clean.dropna(subset=[\"acctdesc\"]).copy()\n",
    "\n",
    "# Optional: convert labels to int (in case they're strings)\n",
    "df_clean['label'] = df_clean['label'].astype(int)\n",
    "\n",
    "# Check balance\n",
    "print(df_clean['label'].value_counts())\n",
    "print(df_clean.head())\n",
    "\n",
    "output_file_path = '../data/userdesc_labeled.csv'\n",
    "df_clean.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add userdesc to intersection for the ensemble model #"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the labeled_intersection.csv file\n",
    "intersection_file_path = '../data/labeled_intersection.csv'\n",
    "df_intersection = pd.read_csv(intersection_file_path)\n",
    "\n",
    "print(\"labeled_intersection.csv loaded successfully. First 5 rows:\")\n",
    "print(df_intersection.head())\n",
    "\n",
    "# Merge df_intersection with df_clean to add the 'acctdesc'\n",
    "# We use a left merge to keep all users from df_intersection\n",
    "df_merged = pd.merge(df_intersection, df_clean[['userid', 'acctdesc']], on='userid', how='right')\n",
    "\n",
    "print(\"\\nMerged DataFrame with 'acctdesc'. First 5 rows:\")\n",
    "print(df_merged.head())\n",
    "\n",
    "# For users in labeled_intersection.csv that were not in df_clean, the 'acctdesc' will be NaN (null)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save to a CSV files #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T18:18:16.921928Z",
     "start_time": "2025-06-04T18:18:15.913571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "proc_df.to_csv('../data/processed_users.csv', index=False)\n",
    "print(\"proc_df saved to '../data/processed_users.csv'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc_df saved to 'data/processed_users.csv'\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:21:17.526519Z",
     "start_time": "2025-06-04T22:21:17.266642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_intersection.to_csv('../data/labeled_intersection.csv', index=False)\n",
    "print(\"df_intersection saved to '../data/labeled_intersection.csv'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_intersection saved to 'data/labeled_intersection.csv'\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_path = '../data/intersection_userdesc_labeled.csv'\n",
    "df_merged.to_csv(output_file_path, index=False)\n"
   ]
  }
 ]
}
