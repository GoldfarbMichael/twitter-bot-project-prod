{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import gc\n"
   ],
   "metadata": {
    "id": "Sk3u7UimFeSc",
    "ExecuteTime": {
     "end_time": "2025-06-04T17:27:58.999786Z",
     "start_time": "2025-06-04T17:27:58.982189Z"
    }
   },
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Combine proc_df with_bot_records_filterd.csv  #\n",
    "* In this case we will insert 0 to each missing value that is added form bot_records_filtered.csv the purpose is to enlarge the dataset with bot records"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T19:03:41.828021Z",
     "start_time": "2025-06-04T19:03:41.744627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the bot-only data\n",
    "df_bot = pd.read_csv('../data/bot_records_filtered.csv')\n",
    "# Load proc_df from 'processed_users.csv'\n",
    "proc_df = pd.read_csv('../data/processed_users.csv')\n",
    "\n",
    "# Load df_bot from your bot file\n",
    "df_bot = pd.read_csv('../data/bot_records_filtered.csv')\n",
    "# Ensure all columns from proc_df exist in df_bot, fill missing with 0\n",
    "for col in proc_df.columns:\n",
    "    if col not in df_bot.columns:\n",
    "        df_bot[col] = 0\n",
    "\n",
    "# Reorder columns to match proc_df\n",
    "df_bot = df_bot[proc_df.columns]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "proc_df = pd.concat([proc_df, df_bot], ignore_index=True)\n",
    "\n",
    "print(\"Merged 'bot_only_filtered.csv' into 'proc_df'.\")\n",
    "print(proc_df.tail())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 'bot_only_filtered.csv' into 'proc_df'.\n",
      "        userid  totaltweets  following  followers  followers_following_ratio  \\\n",
      "129297  366812            0          0       2682                        0.0   \n",
      "129298  533331            0          0       1309                        0.0   \n",
      "129299  491196            0          0       9911                        0.0   \n",
      "129300  739297            0          0       9900                        0.0   \n",
      "129301  674475            0          0       6313                        0.0   \n",
      "\n",
      "        avg_retweetcount  avg_words_per_tweet  daily_tweet_count  \\\n",
      "129297              11.0                  0.0                0.0   \n",
      "129298               6.0                  0.0                0.0   \n",
      "129299              64.0                  0.0                0.0   \n",
      "129300              18.0                  0.0                0.0   \n",
      "129301              43.0                  0.0                0.0   \n",
      "\n",
      "        unique_language_count  Max_tweets_in_1hr  label  \n",
      "129297                      0                0.0      1  \n",
      "129298                      0                0.0      1  \n",
      "129299                      0                0.0      1  \n",
      "129300                      0                0.0      1  \n",
      "129301                      0                0.0      1  \n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T20:26:51.419728Z",
     "start_time": "2025-06-04T20:26:51.252593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add missing columns to df_bot with value 0\n",
    "for col in proc_df.columns:\n",
    "    if col not in df_bot.columns:\n",
    "        df_bot[col] = 0\n",
    "\n",
    "# Reorder df_bot columns to match proc_df\n",
    "df_bot = df_bot[proc_df.columns]\n",
    "\n",
    "print(\"df_bot columns now match proc_df (order and presence).\")\n",
    "print(df_bot.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_bot columns now match proc_df (order and presence).\n",
      "   userid  totaltweets  following  followers  followers_following_ratio  \\\n",
      "0  132131            0          0       2353                          0   \n",
      "1  696168            0          0       2242                          0   \n",
      "2  704441            0          0       8438                          0   \n",
      "3  570928            0          0       3792                          0   \n",
      "4  107312            0          0       1442                          0   \n",
      "\n",
      "   avg_retweetcount  avg_words_per_tweet  daily_tweet_count  \\\n",
      "0                85                    0                  0   \n",
      "1                54                    0                  0   \n",
      "2                26                    0                  0   \n",
      "3                41                    0                  0   \n",
      "4                64                    0                  0   \n",
      "\n",
      "   unique_language_count  Max_tweets_in_1hr  label  \n",
      "0                      0                  0      1  \n",
      "1                      0                  0      1  \n",
      "2                      0                  0      1  \n",
      "3                      0                  0      1  \n",
      "4                      0                  0      1  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create data set that has *only* intersected features  #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:19:57.770037Z",
     "start_time": "2025-06-04T22:19:57.609239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload DataFrames\n",
    "proc_df = pd.read_csv('../data/processed_users.csv')\n",
    "df_bot = pd.read_csv('../data/bot_records_filtered.csv')\n",
    "\n",
    "# Find intersection and keep order as in df_bot\n",
    "common_cols = [col for col in df_bot.columns if col in proc_df.columns]\n",
    "\n",
    "# Subset both DataFrames to common columns\n",
    "proc_df_common = proc_df[common_cols]\n",
    "df_bot_common = df_bot[common_cols]\n",
    "\n",
    "# Concatenate\n",
    "df_intersection = pd.concat([proc_df_common, df_bot_common], ignore_index=True)\n",
    "\n",
    "print(\"Concatenated DataFrame with intersection columns (order as in df_bot):\")\n",
    "print(df_intersection.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated DataFrame with intersection columns (order as in df_bot):\n",
      "               userid  avg_retweetcount  followers  label\n",
      "0            22240612          1.755378     925487      0\n",
      "1             6135622         39.639711    1367996      0\n",
      "2  848416437030985728          1.000000      47826      0\n",
      "3  984429894829592576          0.398857        328      0\n",
      "4  807095565028917248         18.340000      26020      0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save to a CSV files #"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T20:30:12.466045Z",
     "start_time": "2025-06-04T20:30:12.382942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_bot.to_csv('../data/partial_features.csv', index=False)\n",
    "print(\"df_bot saved to '../data/partial_features.csv'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_bot saved to 'data/partial_features.csv'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:21:17.526519Z",
     "start_time": "2025-06-04T22:21:17.266642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_intersection.to_csv('../data/labeled_intersection.csv', index=False)\n",
    "print(\"df_intersection saved to '../data/labeled_intersection.csv'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_intersection saved to 'data/labeled_intersection.csv'\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ]
}
