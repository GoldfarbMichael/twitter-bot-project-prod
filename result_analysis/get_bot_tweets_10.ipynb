{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {
    "id": "Wl1e3u6G6kDQ"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "file_path = '../data/unique_users_after_labeling.csv'\n",
    "labeled_df = pd.read_csv(file_path, usecols=['userid', 'label'])\n",
    "# Define the directory containing the CSV files and the output file path\n",
    "source_directory = \"../data/twitter_proc/files\"\n",
    "output_file = '../data/bot_tweets_by_user.csv'\n",
    "processed_files_report = 'processed_files_report.txt'\n",
    "\n",
    "print(labeled_df.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ir0BMgb_bOX",
    "outputId": "f3c8905d-4702-4a1b-d81a-2dc82482cff7"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               userid  label\n",
      "0            22240612      0\n",
      "1             6135622      0\n",
      "2  848416437030985728      0\n",
      "3  984429894829592576      0\n",
      "4  807095565028917248      0\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Initialize an empty list to store data before appending\n",
    "data_to_append = []\n",
    "rows_processed_count = 0\n",
    "append_threshold = 10000\n",
    "\n",
    "# Keep track of processed files\n",
    "processed_files = set()\n",
    "\n",
    "# Load previously processed files if the report file exists\n",
    "if os.path.exists(processed_files_report):\n",
    "    with open(processed_files_report, 'r') as f:\n",
    "        for line in f:\n",
    "            processed_files.add(line.strip())\n",
    "\n",
    "print(\"Starting file processing...\")\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for filename in os.listdir(source_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(source_directory, filename)\n",
    "\n",
    "        # Skip if the file has already been processed\n",
    "        if filename in processed_files:\n",
    "            print(f\"Skipping already processed file: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing file: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Load the CSV file, only needing 'userid' and 'text'\n",
    "            current_tweets_df = pd.read_csv(file_path, usecols=['userid', 'text'])\n",
    "            merged_df = pd.merge(current_tweets_df, labeled_df[['userid', 'label']], on='userid', how='left')\n",
    "\n",
    "            # Filter for bots (where predicted_label is 1)\n",
    "            bot_tweets = merged_df[merged_df['label'] == 1][['userid', 'text']]\n",
    "\n",
    "            # Append to the list\n",
    "            data_to_append.append(bot_tweets)\n",
    "            rows_processed_count += len(bot_tweets)\n",
    "\n",
    "            # Check if we should append to the output file\n",
    "            if rows_processed_count >= append_threshold:\n",
    "                print(f\"Appending {rows_processed_count} rows to output file...\")\n",
    "                combined_df = pd.concat(data_to_append, ignore_index=True)\n",
    "\n",
    "                # Check if the output file already exists to handle headers\n",
    "                if not os.path.exists(output_file):\n",
    "                    combined_df.to_csv(output_file, index=False, mode='w')\n",
    "                else:\n",
    "                    combined_df.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "                # Clear the list and reset count after appending\n",
    "                data_to_append = []\n",
    "                rows_processed_count = 0\n",
    "\n",
    "                # Record processed files\n",
    "                with open(processed_files_report, 'a') as f:\n",
    "                    f.write(filename + '\\n')\n",
    "                processed_files.add(filename)\n",
    "                print(f\"Appended data and recorded {filename} as processed.\")\n",
    "\n",
    "        except KeyError as ke:\n",
    "            print(f\"Error processing {filename}: Missing required column. {ke}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Append any remaining data in the list\n",
    "if data_to_append:\n",
    "    print(f\"Appending remaining {rows_processed_count} rows to output file...\")\n",
    "    combined_df = pd.concat(data_to_append, ignore_index=True)\n",
    "    if not os.path.exists(output_file):\n",
    "        combined_df.to_csv(output_file, index=False, mode='w')\n",
    "    else:\n",
    "        combined_df.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "    # Record remaining processed files\n",
    "    with open(processed_files_report, 'a') as f:\n",
    "        with open(processed_files_report, 'r') as report_check_f:\n",
    "            processed_lines = set(line.strip() for line in report_check_f)\n",
    "        for filename in processed_files:\n",
    "            if filename not in processed_lines:\n",
    "                 f.write(filename + '\\n')\n",
    "    print(\"Appended remaining data.\")\n",
    "\n",
    "\n",
    "# Final step: Load the complete file and sort by userid\n",
    "if os.path.exists(output_file):\n",
    "    print(\"Loading the complete output file for sorting...\")\n",
    "    final_df = pd.read_csv(output_file)\n",
    "    print(\"Sorting by userid...\")\n",
    "    final_df_sorted = final_df.sort_values(by='userid')\n",
    "\n",
    "    # Overwrite the output file with the sorted data\n",
    "    print(\"Saving the final sorted file...\")\n",
    "    final_df_sorted.to_csv(output_file, index=False)\n",
    "    print(\"Processing complete. Final sorted data saved.\")\n",
    "else:\n",
    "    print(\"No data was processed.\")"
   ],
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwcDtjB7CGVg",
    "outputId": "47dbbc56-5cea-47ff-83f9-1c3685ae0462"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting file processing...\n",
      "Appending 10309 rows to output file...\n",
      "Appended data and recorded 20230421_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11697 rows to output file...\n",
      "Appended data and recorded 20230610_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10746 rows to output file...\n",
      "Appended data and recorded 20230407_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11033 rows to output file...\n",
      "Appended data and recorded 1008_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10497 rows to output file...\n",
      "Appended data and recorded 1106_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11478 rows to output file...\n",
      "Appended data and recorded 20230305_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10094 rows to output file...\n",
      "Appended data and recorded 1023_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10210 rows to output file...\n",
      "Appended data and recorded 20230213_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11787 rows to output file...\n",
      "Appended data and recorded 20230314_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12766 rows to output file...\n",
      "Appended data and recorded 20230224_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 13470 rows to output file...\n",
      "Appended data and recorded 20230306_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11420 rows to output file...\n",
      "Appended data and recorded 1227_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10260 rows to output file...\n",
      "Appended data and recorded 20230612_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10139 rows to output file...\n",
      "Appended data and recorded 1107_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11472 rows to output file...\n",
      "Appended data and recorded 0824_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11280 rows to output file...\n",
      "Appended data and recorded 20230604_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 13428 rows to output file...\n",
      "Appended data and recorded 20230226_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11822 rows to output file...\n",
      "Appended data and recorded 20230317_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11595 rows to output file...\n",
      "Appended data and recorded 1222_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10393 rows to output file...\n",
      "Appended data and recorded 20230130_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11391 rows to output file...\n",
      "Appended data and recorded 20230116_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10410 rows to output file...\n",
      "Appended data and recorded 1003_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10268 rows to output file...\n",
      "Appended data and recorded 20230424_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11582 rows to output file...\n",
      "Appended data and recorded 20230412_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10560 rows to output file...\n",
      "Appended data and recorded 20230204_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11944 rows to output file...\n",
      "Appended data and recorded 20230319_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12403 rows to output file...\n",
      "Appended data and recorded 20230402_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11034 rows to output file...\n",
      "Appended data and recorded 1231_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10604 rows to output file...\n",
      "Appended data and recorded 1105_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10821 rows to output file...\n",
      "Appended data and recorded 20230221_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10486 rows to output file...\n",
      "Appended data and recorded 1119_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10288 rows to output file...\n",
      "Appended data and recorded 0929_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11176 rows to output file...\n",
      "Appended data and recorded 0825_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12049 rows to output file...\n",
      "Appended data and recorded 20230327_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12758 rows to output file...\n",
      "Appended data and recorded 20230323_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10274 rows to output file...\n",
      "Appended data and recorded 20230220_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 13835 rows to output file...\n",
      "Appended data and recorded 1029_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 13958 rows to output file...\n",
      "Appended data and recorded 20230307_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10709 rows to output file...\n",
      "Appended data and recorded 0914_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 13130 rows to output file...\n",
      "Appended data and recorded 20230321_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10627 rows to output file...\n",
      "Appended data and recorded 0905_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10378 rows to output file...\n",
      "Appended data and recorded 20230419_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12057 rows to output file...\n",
      "Appended data and recorded 20230608_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12307 rows to output file...\n",
      "Appended data and recorded 20230313_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 10327 rows to output file...\n",
      "Appended data and recorded 20230103_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11011 rows to output file...\n",
      "Appended data and recorded 20230227_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 12108 rows to output file...\n",
      "Appended data and recorded 20230329_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11740 rows to output file...\n",
      "Appended data and recorded 20230505_to_20230508_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending 11533 rows to output file...\n",
      "Appended data and recorded 1004_UkraineCombinedTweetsDeduped.csv as processed.\n",
      "Appending remaining 7087 rows to output file...\n",
      "Appended remaining data.\n",
      "Loading the complete output file for sorting...\n",
      "Sorting by userid...\n",
      "Saving the final sorted file...\n",
      "Processing complete. Final sorted data saved.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bot_text = pd.read_csv(output_file)\n",
    "print(bot_text.shape)\n",
    "print(bot_text.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhS5zVC8Fw0b",
    "outputId": "4ca93766-b2db-4348-cc2d-2a53d050d156"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(380119, 2)\n",
      "   userid                                               text\n",
      "0    1968  @VeritasVinnie21 @MrChuckD They traded her fre...\n",
      "1    1968  @gloria_sin It's not the weapons, its avoiding...\n",
      "2   59563  Finally!!! #Messi𓃵 ❤️❤️❤️ #WorldCupFinal #Arge...\n",
      "3  647943  ++ Ecco i partigiani #Russia anti #Putin. Ora ...\n",
      "4  647943  Sacrificio estremo degli ucraini, attacchi sui...\n"
     ]
    }
   ]
  }
 ]
}
