{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:32:09.734558Z",
     "start_time": "2025-06-10T17:31:50.410839Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import importlib\n",
    "import Ensemble_model.ensemble\n",
    "importlib.reload(Ensemble_model.ensemble)\n",
    "from Ensemble_model.ensemble import BotEnsemble\n",
    "import torch\n",
    "\n",
    "# Automatically use GPU if available, fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load transformers data #",
   "id": "76af32998547d219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:32:15.647898Z",
     "start_time": "2025-06-10T17:32:10.365558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer_path = \"../../twitter-bot-project - Copy/userdesc-LM-model/trained-model/checkpoint-18441\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_path, use_fast=True)\n",
    "\n",
    "transformer_model = AutoModelForSequenceClassification.from_pretrained(transformer_path)\n",
    "transformer_model.eval().to(device)"
   ],
   "id": "2ec557ce0d29a499",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load random forest model #",
   "id": "cd9a5cf265812663"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:32:35.798849Z",
     "start_time": "2025-06-10T17:32:34.292227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# === Load ONNX model ===\n",
    "onnx_model_path = '../../twitter-bot-project - Copy/Numeric_Features_model/trained-model/rf_model.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path, providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "# === Prepare input (test) ===\n",
    "features = [[500, 1.3]]  # Single record with 2 features\n",
    "input_array = np.array(features, dtype=np.float32)\n",
    "\n",
    "# ONNX input name may vary; get it programmatically:\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# === Run inference ===\n",
    "outputs = session.run(None, {input_name: input_array})\n",
    "\n",
    "\n",
    "probs = outputs[1][0]  # [prob_class_0, prob_class_1]\n",
    "prob = probs[1]\n",
    "pred = int(prob > 0.5)\n",
    "\n",
    "print(f\"Predicted class: {pred} (probability: {prob:.4f})\")"
   ],
   "id": "3952a6aa1651621c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1 (probability: 0.5525)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:32:51.488959Z",
     "start_time": "2025-06-10T17:32:51.467918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_threshold(y_true, y_probs, metric=f1_score, step=0.01):\n",
    "    \"\"\"\n",
    "    Finds the best threshold for converting probabilities to labels.\n",
    "    Returns:\n",
    "        best_threshold: Threshold with highest metric score.\n",
    "        best_score: The corresponding metric score.\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0, 1 + step, step)\n",
    "    best_score = -1\n",
    "    best_threshold = 0.5\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_probs >= t).astype(int)\n",
    "        score = metric(y_true, y_pred)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = t\n",
    "    return best_threshold, best_score"
   ],
   "id": "707a0f6725a4ca0d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:32:57.082066Z",
     "start_time": "2025-06-10T17:32:56.586064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize ensemble with random forest model\n",
    "ensemble = BotEnsemble(\n",
    "    transformer_model=transformer_model,\n",
    "    tokenizer=tokenizer,\n",
    "    numeric_model=session,\n",
    "    alpha=1,\n",
    ")\n",
    "record = {\n",
    "    \"followers\": 500,\n",
    "    \"avg_retweetcount\": 1.3,\n",
    "    \"acctdesc\": \"Co-Founder @templatenb #WordPress #Webdevelopment #WooCommerce\"\n",
    "}\n",
    "\n",
    "\n",
    "features = [record[\"followers\"], record[\"avg_retweetcount\"]]\n",
    "prob = ensemble.predict_prob(record[\"acctdesc\"], features)\n",
    "pred = int(prob > 0.5)\n",
    "print(f\"Predicted class: {pred} (probability: {prob:.4f})\\n\\n\")"
   ],
   "id": "2ba12ab4741fa6f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1 (probability: 0.9998)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create filtered users dataset #",
   "id": "1271e6cd6e5aa3ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "# Load the CSV\n",
    "df = pd.read_csv('../data/processed_users.csv')\n",
    "# Keep only the desired columns\n",
    "df = df[['userid', 'followers', 'avg_retweetcount', 'label']]\n",
    "\n",
    "file_path = '../data/userdesc_labeled.csv'\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_desc = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing descriptions\n",
    "df_clean = df_desc.dropna(subset=[\"acctdesc\"]).copy()\n",
    "\n",
    "# Check balance\n",
    "print(df_clean['label'].value_counts())\n",
    "print(df_clean.head())\n",
    "\n",
    "df_merged = pd.merge(df, df_clean[['userid', 'acctdesc']], on='userid', how='left')\n",
    "\n",
    "print(\"\\nMerged DataFrame with 'acctdesc'. First 5 rows:\")\n",
    "print(df_merged.head())\n",
    "df_merged.to_csv('../data/processed_users_filtered.csv', index=False)"
   ],
   "id": "8580741bdb0cb43b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test ensemble with random forest on full data #",
   "id": "36304dbfbdec3860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:01:06.105384Z",
     "start_time": "2025-06-09T11:41:37.634511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# === 1. Load Data ===\n",
    "df = pd.read_csv(\"../data/processed_users_filtered.csv\")\n",
    "\n",
    "# === 2. Drop rows with missing essential values (if any) ===\n",
    "df = df.dropna(subset=[\"followers\", \"avg_retweetcount\", \"label\"])\n",
    "\n",
    "# === 3. Iterate and Predict ===\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    features = [row[\"followers\"], row[\"avg_retweetcount\"]]\n",
    "    desc = row[\"acctdesc\"]\n",
    "    prob = ensemble.predict_prob(desc, features)\n",
    "\n",
    "    y_probs.append(prob)\n",
    "    y_true.append(row[\"label\"])\n",
    "\n",
    "# === 4. Threshold and Metrics ===\n",
    "best_threshold, best_score = find_best_threshold(np.array(y_true), np.array(y_probs), metric=f1_score, step=0.01)\n",
    "print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"Best F1 score: {best_score:.4f}\")\n",
    "\n",
    "y_pred = (np.array(y_probs) >= best_threshold).astype(int)\n",
    "\n",
    "# === 5. Print Metrics ===\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_true, y_probs))"
   ],
   "id": "fc7edef13075da25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.980\n",
      "Best F1 score: 0.8414\n",
      "Confusion Matrix:\n",
      "[[97415   435]\n",
      " [ 1447  4991]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9956    0.9904     97850\n",
      "           1     0.9198    0.7752    0.8414      6438\n",
      "\n",
      "    accuracy                         0.9820    104288\n",
      "   macro avg     0.9526    0.8854    0.9159    104288\n",
      "weighted avg     0.9813    0.9820    0.9812    104288\n",
      "\n",
      "ROC AUC Score: 0.9871173464973793\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Labling all the unlabeled users using ensemble with random forest #",
   "id": "dcd2bf2fcf95fdff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:03:20.207672Z",
     "start_time": "2025-06-10T19:03:08.678874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd# import pandas as pd\n",
    "labeled_path = '../data/unique_users_after_labeling.csv'\n",
    "unlabeled_path = '../data/unique_users_no_intersection_unlabeled.csv'\n",
    "\n",
    "df_labeled = pd.read_csv(labeled_path)\n",
    "df_unlabeled = pd.read_csv(unlabeled_path)\n",
    "\n",
    "# Remove intersection based on 'user_id'\n",
    "remaining = df_unlabeled[~df_unlabeled['userid'].isin(df_labeled['userid'])]\n",
    "print(f\"Total unlabeled users: {len(df_unlabeled)}\")\n",
    "print(f\"Remaining unlabeled users: {len(remaining)}\")\n",
    "print(f\" labeled users: {len(df_labeled)}\")\n",
    "# remaining.to_csv('remaining.csv', index=False)\n",
    "del df_labeled\n",
    "del df_unlabeled\n",
    "del remaining"
   ],
   "id": "b44bf7bc657f0399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unlabeled users: 2285391\n",
      "Remaining unlabeled users: 0\n",
      " labeled users: 2285391\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:34:54.833576Z",
     "start_time": "2025-06-10T17:36:20.331263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd# import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "labeled_path = '../data/unique_users_after_labeling.csv'\n",
    "unlabeled_path = '../data/unique_users_no_intersection_unlabeled.csv'\n",
    "\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "reader = pd.read_csv(unlabeled_path, chunksize=batch_size)\n",
    "first_batch = False\n",
    "\n",
    "for chunk in tqdm(reader, desc=\"Batch labeling\"):\n",
    "    chunk = chunk.dropna(subset=[\"followers\", \"avg_retweetcount\"])\n",
    "    chunk[\"predicted_label\"] = chunk.apply(\n",
    "        lambda row: ensemble.predict_label(\n",
    "            features=[row[\"followers\"], row[\"avg_retweetcount\"]],\n",
    "            acctdesc=row[\"acctdesc\"],\n",
    "            threshold=0.98\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    chunk.to_csv(labeled_path, mode='w' if first_batch else 'a', index=False, header=first_batch)\n",
    "    first_batch = False\n"
   ],
   "id": "1777d1269cc83a3c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch labeling: 105it [58:34, 33.47s/it]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:05:47.583018Z",
     "start_time": "2025-06-10T19:05:41.630685Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of bots (1) and humans (0):\n",
      "predicted_label\n",
      "0    2226874\n",
      "1      58517\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data\n",
    "df = pd.read_csv('../data/unique_users_after_labeling.csv')\n",
    "\n",
    "# Get the distribution of bots (1) and humans (0)\n",
    "label_counts = df['predicted_label'].value_counts()\n",
    "print(\"Distribution of bots (1) and humans (0):\")\n",
    "print(label_counts)\n",
    "del label_counts\n",
    "del df"
   ],
   "id": "a7d5175b37bcea08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Add to unique_users_after_labeling.csv the prelabeled users  #\n",
    "\n",
    "* First step is to get all the labeled users and merge \"acctdesc\" with the numeric features"
   ],
   "id": "c637efb89214da1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import display\n",
    "# Define file paths\n",
    "processed_users_path = '../data/processed_users.csv'\n",
    "userdesc_labeled_path = '../data/userdesc_labeled.csv'\n",
    "\n",
    "\n",
    "# Define the columns you want from processed_users_df *after* dropping\n",
    "processed_users_cols_to_keep = ['userid', 'totaltweets', 'avg_retweetcount', 'followers', 'following', 'label']\n",
    "userdesc_labeled_cols_to_keep = ['userid', 'acctdesc']\n",
    "\n",
    "# Load the datasets and select/drop columns immediately\n",
    "try:\n",
    "    processed_users_df = pd.read_csv(processed_users_path)[processed_users_cols_to_keep]\n",
    "    userdesc_labeled_df = pd.read_csv(userdesc_labeled_path)[userdesc_labeled_cols_to_keep]\n",
    "\n",
    "    print(\"DataFrames loaded and relevant columns selected.\")\n",
    "    print(\"Processed Users Shape (selected columns):\", processed_users_df.shape)\n",
    "    print(\"Userdesc Labeled Shape (selected columns):\", userdesc_labeled_df.shape)\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        processed_users_df,\n",
    "        userdesc_labeled_df,\n",
    "        on='userid',\n",
    "        how='left',\n",
    "    )\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    current_columns = merged_df.columns.tolist()\n",
    "    acctdesc_index = current_columns.index('acctdesc')\n",
    "    label_index = current_columns.index('label')\n",
    "\n",
    "    # Swap their positions in the list\n",
    "    current_columns[acctdesc_index], current_columns[label_index] = current_columns[label_index], current_columns[acctdesc_index]\n",
    "\n",
    "    # Reindex the DataFrame with the new column order\n",
    "    merged_df_reordered = merged_df[current_columns]\n",
    "    print(\"\\nDataFrames merged successfully on 'userid'.\")\n",
    "    print(\"Merged DataFrame Shape:\", merged_df.shape)\n",
    "    display(merged_df_reordered.head()) # Use display for better output formatting\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: One of the files was not found. {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column specified for selecting or reordering was not found. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ],
   "id": "1abcdee23dd0d046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_users_path = '../data/unique_users_after_labeling.csv'\n",
    "\n",
    "try:\n",
    "    # Load the unique users file\n",
    "    unique_users_df = pd.read_csv(unique_users_path)\n",
    "    print(f\"\\nLoaded unique users file: {unique_users_path}\")\n",
    "    print(\"Unique Users Shape (before modifications):\", unique_users_df.shape)\n",
    "    print(\"Unique Users Columns (before modifications):\", unique_users_df.columns.tolist())\n",
    "\n",
    "    # Drop the \"count\" column\n",
    "    if 'count' in unique_users_df.columns:\n",
    "        unique_users_df = unique_users_df.drop(columns=['count'])\n",
    "        print(\"Dropped 'count' column from unique_users_df.\")\n",
    "    else:\n",
    "        print(\"'count' column not found in unique_users_df. Skipping drop.\")\n",
    "\n",
    "    # Rename \"predicted_label\" to \"label\"\n",
    "    if 'predicted_label' in unique_users_df.columns:\n",
    "        unique_users_df = unique_users_df.rename(columns={'predicted_label': 'label'})\n",
    "        print(\"Renamed 'predicted_label' to 'label' in unique_users_df.\")\n",
    "    else:\n",
    "         print(\"'predicted_label' column not found in unique_users_df. Skipping rename.\")\n",
    "\n",
    "    common_columns = list(merged_df_reordered.columns) # Get the column names from merged_df\n",
    "\n",
    "    columns_to_select = [col for col in common_columns if col in unique_users_df.columns]\n",
    "    unique_users_df_aligned = unique_users_df[columns_to_select]\n",
    "\n",
    "\n",
    "    print(\"\\nUnique Users Shape (after modifications and alignment):\", unique_users_df_aligned.shape)\n",
    "    print(\"Unique Users Columns (after modifications and alignment):\", unique_users_df_aligned.columns.tolist())\n",
    "    print(\"Merged DataFrame Columns:\", merged_df_reordered.columns.tolist())\n",
    "\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    # Use ignore_index=True to reset the index after concatenation\n",
    "    combined_df = pd.concat([merged_df_reordered, unique_users_df_aligned], ignore_index=True)\n",
    "\n",
    "    print(\"\\nDataFrames concatenated successfully.\")\n",
    "    print(\"Combined DataFrame Shape:\", combined_df.shape)\n",
    "    print(\"Combined DataFrame Head:\")\n",
    "    display(combined_df.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error processing unique_users_df: The file was not found. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while processing unique_users_df: {e}\")\n",
    "\n",
    "print(\"\\nLabel Counts in combined_df:\")\n",
    "label_counts = combined_df['label'].value_counts()\n",
    "print(label_counts)\n",
    "combined_df.to_csv(unique_users_path, index=False, mode='w')"
   ],
   "id": "1b2c163e6f19eecc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
